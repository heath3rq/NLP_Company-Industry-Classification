{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinkedIn Company Industry Classification based on Company Description\n",
    "\n",
    "Submitted by Ya Yun Huang, Emma Wang, Heather Qiu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Dependencies\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import data from csv\n",
    "df = pd.read_csv(\"linkedin_company_data.csv\", encoding=\"latin1\")\n",
    "## Confirm all fields have a value\n",
    "assert ((df.isnull() == False).all()).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hospitals and Health Care        946\n",
       "Financial Services               926\n",
       "IT Services and IT Consulting    833\n",
       "Name: industry, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Confirm that the dataset is balanced among classes\n",
    "df[\"industry\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is balanced across different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Financial Services': 0, 'Hospitals and Health Care': 1, 'IT Services and IT Consulting': 2}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>industry</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3M Health Care</td>\n",
       "      <td>We believe nothing is more important than good...</td>\n",
       "      <td>Hospitals and Health Care</td>\n",
       "      <td>1</td>\n",
       "      <td>believe nothing important good health belief i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AHIMA</td>\n",
       "      <td>AHIMA is a global nonprofit association of hea...</td>\n",
       "      <td>Hospitals and Health Care</td>\n",
       "      <td>1</td>\n",
       "      <td>ahima global nonprofit association health info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Allina Health</td>\n",
       "      <td>People at Allina Health have a career of makin...</td>\n",
       "      <td>Hospitals and Health Care</td>\n",
       "      <td>1</td>\n",
       "      <td>people allina health career making difference ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>American College of Cardiology</td>\n",
       "      <td>The American College of Cardiology envisions a...</td>\n",
       "      <td>Hospitals and Health Care</td>\n",
       "      <td>1</td>\n",
       "      <td>american college cardiology envisions world in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apria</td>\n",
       "      <td>Apria is a leading provider of home healthcare...</td>\n",
       "      <td>Hospitals and Health Care</td>\n",
       "      <td>1</td>\n",
       "      <td>apria leading provider home healthcare equipme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             name  \\\n",
       "0                  3M Health Care   \n",
       "1                           AHIMA   \n",
       "2                   Allina Health   \n",
       "3  American College of Cardiology   \n",
       "4                           Apria   \n",
       "\n",
       "                                         description  \\\n",
       "0  We believe nothing is more important than good...   \n",
       "1  AHIMA is a global nonprofit association of hea...   \n",
       "2  People at Allina Health have a career of makin...   \n",
       "3  The American College of Cardiology envisions a...   \n",
       "4  Apria is a leading provider of home healthcare...   \n",
       "\n",
       "                    industry  label  \\\n",
       "0  Hospitals and Health Care      1   \n",
       "1  Hospitals and Health Care      1   \n",
       "2  Hospitals and Health Care      1   \n",
       "3  Hospitals and Health Care      1   \n",
       "4  Hospitals and Health Care      1   \n",
       "\n",
       "                                                text  \n",
       "0  believe nothing important good health belief i...  \n",
       "1  ahima global nonprofit association health info...  \n",
       "2  people allina health career making difference ...  \n",
       "3  american college cardiology envisions world in...  \n",
       "4  apria leading provider home healthcare equipme...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode labels to numbers\n",
    "le = LabelEncoder()\n",
    "df[\"label\"] = le.fit_transform(df[\"industry\"])\n",
    "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(le_name_mapping)\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Text Preprocessing Function\n",
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "\n",
    "    ## Remove unicode characters in company descriptions\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "\n",
    "    ## Convert company descriptions to lowercase\n",
    "    ## and remove punctuations and characters and then strip\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", str(text).lower().strip())\n",
    "\n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()\n",
    "\n",
    "    ## Remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in lst_stopwords]\n",
    "\n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "\n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "\n",
    "    ## Convert lists back to strings\n",
    "    text = \" \".join(lst_text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Preprocess LinkedIn Company Description\n",
    "# specify stopwords\n",
    "lst_stopwords = nltk.corpus.stopwords.words(\"english\") + [\n",
    "    \"said\",\n",
    "    \"mr\",\n",
    "    \"could\",\n",
    "    \"doe\",\n",
    "    \"ha\",\n",
    "    \"might\",\n",
    "    \"must\",\n",
    "    \"need\",\n",
    "    \"sha\",\n",
    "    \"wa\",\n",
    "    \"wo\",\n",
    "    \"would\",\n",
    "]\n",
    "\n",
    "df[\"text\"] = df[\"description\"].apply(\n",
    "    lambda x: utils_preprocess_text(\n",
    "        x, flg_stemm=False, flg_lemm=True, lst_stopwords=lst_stopwords\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the dataset into test (20%) and train (80%)\n",
    "train_X, test_X, train_y, test_y = train_test_split(\n",
    "    df[\"text\"], df[\"label\"], test_size=0.2, random_state=1\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Train Multinomial Naive Bayes Model (Generative) on Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the multinomial naive bayes model on original data is 0.93.\n",
      "                               precision    recall  f1-score   support\n",
      "\n",
      "           Financial Services       0.94      0.91      0.92       175\n",
      "    Hospitals and Health Care       0.96      0.93      0.94       188\n",
      "IT Services and IT Consulting       0.88      0.94      0.91       178\n",
      "\n",
      "                     accuracy                           0.93       541\n",
      "                    macro avg       0.93      0.93      0.93       541\n",
      "                 weighted avg       0.93      0.93      0.93       541\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Instantiate the Count Vectorizer Object\n",
    "count = CountVectorizer(lowercase=True, min_df=2, max_df=0.9, ngram_range=(1, 1))\n",
    "train_count_X = count.fit_transform(train_X)\n",
    "test_count_X = count.transform(test_X)\n",
    "\n",
    "# Instantiate the multinomial model and fit the model\n",
    "mod_nb = MultinomialNB()\n",
    "fit_model = mod_nb.fit(train_count_X, train_y.values.ravel())\n",
    "\n",
    "## Make predictions using test set\n",
    "pred_y = fit_model.predict(test_count_X)\n",
    "\n",
    "## Evaluate accuracy\n",
    "print(\n",
    "    f\"The accuracy of the multinomial naive bayes model on original data is {accuracy_score(test_y, list(pred_y)):,.2f}.\"\n",
    ")\n",
    "# print out classification report\n",
    "target_names = ['Financial Services', 'Hospitals and Health Care', 'IT Services and IT Consulting']\n",
    "print(classification_report(test_y, pred_y, target_names=target_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Train Pre-trained BERT Model (Discriminative) from Hugging Face on Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Additional Dependencies\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    TFAutoModelForSequenceClassification,\n",
    "    TFBertForSequenceClassification,\n",
    ")\n",
    "from transformers import AutoTokenizer\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizerFast\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def get_train_ds(train_X, train_y, batch_size=32):\n",
    "    X_train, y_train = train_X, train_y\n",
    "    train_encodings = tokenizer(list(train_X), truncation=True, padding=True)\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train))\n",
    "\n",
    "    train_ds = train_ds.batch(batch_size)\n",
    "    return train_ds\n",
    "\n",
    "\n",
    "## Train and save model\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=3)\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=5e-5, decay_steps=10000, decay_rate=0.9\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
    ")\n",
    "\n",
    "\n",
    "def train_on_slice(train_ds):\n",
    "    model.fit(train_ds, epochs=1)\n",
    "    tokenizer.save_pretrained(\"./trained\")\n",
    "    model.save_pretrained(\"./trained\")\n",
    "\n",
    "\n",
    "train_ds = get_train_ds(train_X, train_y)\n",
    "train_on_slice(train_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Additional Dependencies\n",
    "from transformers import BertTokenizerFast, TFBertForSequenceClassification\n",
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "## Load and Predict using saved model\n",
    "model_path = \"./trained\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "model = TFBertForSequenceClassification.from_pretrained(\n",
    "    model_path\n",
    ")  # modify labels as needed.\n",
    "text = list(test_X)\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "ans = [int(i[\"label\"][-1]) for i in pipe(text, truncation=True)]\n",
    "\n",
    "## Calculate Accuracy Score\n",
    "print(\n",
    "    f\"The accuracy of the pre-trained BERT model on original data is {accuracy_score(list(test_y), ans):,.2f}.\"\n",
    ")\n",
    "## print classification report\n",
    "print(classification_report(test_y, ans, target_names=target_names))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Generate Synthesize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocabulary used as features in our NB model\n",
    "vocab = count.get_feature_names_out()\n",
    "dims = len(vocab)\n",
    "\n",
    "# get probability distribution of each feature of NB model\n",
    "dist = {}\n",
    "for i in range(0,3):\n",
    "    dist[str(i)] =  np.exp(fit_model.feature_log_prob_[i])\n",
    "\n",
    "X = np.empty((0, dims))\n",
    "y = []\n",
    "\n",
    "# specify text length as 100 (100 words for each text)\n",
    "n = 100\n",
    "all_text = []\n",
    "\n",
    "# generate 300 texts for each class\n",
    "synth_size = 300\n",
    "\n",
    "for label in dist.keys():\n",
    "    synth_text = []  \n",
    "    X_synth = np.random.multinomial(n, dist[label], synth_size)\n",
    "    for i in range(synth_size):\n",
    "        indexlist = []\n",
    "        l = X_synth[i].tolist()\n",
    "        for i,val in enumerate(l):\n",
    "            if val>=1:\n",
    "                indexlist.append(i)\n",
    "        text = []\n",
    "        for index, value in enumerate(vocab):\n",
    "            if index in indexlist:\n",
    "                text.append(value)\n",
    "        synth_text.append(text)\n",
    "\n",
    "    all_text.append(synth_text)\n",
    "    \n",
    "    X = np.concatenate((X, X_synth), axis=0)\n",
    "    yi = np.array([int(label)] * synth_size)\n",
    "    y = np.concatenate((y, yi), axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generative text data\n",
    "synthetic_text = []\n",
    "for label in all_text:\n",
    "    for synth_text in label:\n",
    "        synth_text = \" \".join(synth_text)\n",
    "        synthetic_text.append(synth_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the sytheic dataset into test (20%) and train (80%)\n",
    "train_syn_X, test_syn_X, train_syn_y, test_syn_y = train_test_split(\n",
    "    synthetic_text, y, test_size=0.2, random_state=1\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Train Multinomial Naive Bayes Model (Generative) on Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the multinomial naive bayes model on syntheic data is 1.00.\n",
      "                               precision    recall  f1-score   support\n",
      "\n",
      "           Financial Services       1.00      1.00      1.00        60\n",
      "    Hospitals and Health Care       1.00      1.00      1.00        55\n",
      "IT Services and IT Consulting       1.00      1.00      1.00        65\n",
      "\n",
      "                     accuracy                           1.00       180\n",
      "                    macro avg       1.00      1.00      1.00       180\n",
      "                 weighted avg       1.00      1.00      1.00       180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_count_syn_X = count.fit_transform(train_syn_X)\n",
    "test_count_syn_X = count.transform(test_syn_X)\n",
    "\n",
    "# Instantiate the multinomial model and fit the model\n",
    "mod_nb_syn = MultinomialNB()\n",
    "fit_model_syn = mod_nb_syn.fit(train_count_syn_X, train_syn_y)\n",
    "\n",
    "## Make predictions using test set\n",
    "pred_y_syn = fit_model_syn.predict(test_count_syn_X)\n",
    "\n",
    "## Evaluate accuracy\n",
    "print(\n",
    "    f\"The accuracy of the multinomial naive bayes model on syntheic data is {accuracy_score(test_syn_y, list(pred_y_syn)):,.2f}.\"\n",
    ")\n",
    "# print out classification report\n",
    "target_names = ['Financial Services', 'Hospitals and Health Care', 'IT Services and IT Consulting']\n",
    "print(classification_report(test_syn_y, pred_y_syn, target_names=target_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Train Pre-trained Bert Model (Discriminative) from Hugging Face on Synthetic Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at /workspaces/assimilate-pytorch/trained were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /workspaces/assimilate-pytorch/trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the pre-trained BERT model on synthetic data is 0.87.\n"
     ]
    }
   ],
   "source": [
    "## Import Additional Dependencies\n",
    "from transformers import BertTokenizerFast, TFBertForSequenceClassification\n",
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "## Load and Predict using saved model\n",
    "# model_path = \"./trained\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"/workspaces/assimilate-pytorch/trained\")\n",
    "model = TFBertForSequenceClassification.from_pretrained(\n",
    "    \"/workspaces/assimilate-pytorch/trained\"\n",
    ")  # modify labels as needed.\n",
    "\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "ans = [int(i[\"label\"][-1]) for i in pipe(synthetic_text, truncation=True)]\n",
    "\n",
    "## Calculate Accuracy Score\n",
    "print(\n",
    "    f\"The accuracy of the pre-trained BERT model on synthetic data is {accuracy_score(list(y), ans):,.2f}.\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ebe44062881841b02cd050ef57f0aeb6389a9f1d11eda774fd172b536569d7ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
